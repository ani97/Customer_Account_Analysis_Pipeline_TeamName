{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71c64ca1-64ff-4dca-8b40-ded810517077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Schema function\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def get_schemas():\n",
    "    return {\n",
    "        'Accounts': StructType([\n",
    "            StructField('account_id', IntegerType(), True),\n",
    "            StructField('customer_id', IntegerType(), True),\n",
    "            StructField('account_type', StringType(), True),\n",
    "            StructField('balance', DoubleType(), True)\n",
    "        ]),\n",
    "        'Customers': StructType([\n",
    "            StructField('customer_id', IntegerType(), True),\n",
    "            StructField('first_name', StringType(), True),\n",
    "            StructField('last_name', StringType(), True),\n",
    "            StructField('address', StringType(), True),\n",
    "            StructField('city', StringType(), True),\n",
    "            StructField('state', StringType(), True),\n",
    "            StructField('zip', StringType(), True)\n",
    "        ]),\n",
    "        'Loan_Payments': StructType([\n",
    "            StructField('payment_id', IntegerType(), True),\n",
    "            StructField('loan_id', IntegerType(), True),\n",
    "            StructField('payment_date', DateType(), True),\n",
    "            StructField('payment_amount', DoubleType(), True)\n",
    "        ]),\n",
    "        'Loans': StructType([\n",
    "            StructField('loan_id', IntegerType(), True),\n",
    "            StructField('customer_id', IntegerType(), True),\n",
    "            StructField('loan_amount', DoubleType(), True),\n",
    "            StructField('interest_rate', DoubleType(), True),\n",
    "            StructField('loan_term', IntegerType(), True)\n",
    "        ]),\n",
    "        'Transactions': StructType([\n",
    "            StructField('transaction_id', IntegerType(), True),\n",
    "            StructField('account_id', IntegerType(), True),\n",
    "            StructField('transaction_date', DateType(), True),\n",
    "            StructField('transaction_amount', DoubleType(), True),\n",
    "            StructField('transaction_type', StringType(), True)\n",
    "        ])\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4afb84c6-dc0a-4f83-b9d8-5685999458ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. CHeck nulls and fill nulls value\n",
    "def clean_data(df, file_type):\n",
    "    if file_type == 'Accounts':\n",
    "        return df.filter(\n",
    "            (col('account_id').isNotNull()) | (col('customer_id').isNotNull())\n",
    "        ).na.fill({'account_type': 'Unknown', 'balance': 0.0})\n",
    "    \n",
    "    elif file_type == 'Customers':\n",
    "        return df.filter(\n",
    "            col('customer_id').isNotNull()\n",
    "        ).na.fill({'first_name': 'Missing', 'last_name': 'Missing', 'address': 'Unknown',\n",
    "                   'city': 'Unknown', 'state': 'Unknown', 'zip': '000000'})\n",
    "    \n",
    "    elif file_type == 'Loan_Payments':\n",
    "        return df.filter(\n",
    "            (col('payment_id').isNotNull()) | (col('loan_id').isNotNull())\n",
    "        ).na.fill({'payment_date': '1900-01-01', 'payment_amount': -1.0})\n",
    "    \n",
    "    elif file_type == 'Loans':\n",
    "        return df.filter(\n",
    "            (col('loan_id').isNotNull()) | (col('customer_id').isNotNull())\n",
    "        ).na.fill({'loan_amount': -1.0, 'interest_rate': -1.0, 'loan_term': -1})\n",
    "    \n",
    "    elif file_type == 'Transactions':\n",
    "        return df.filter(\n",
    "            (col('transaction_id').isNotNull()) | (col('account_id').isNotNull())\n",
    "        ).na.fill({'transaction_date': '1900-01-01', 'transaction_amount': -1.0, 'transaction_type': 'Unknown'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f90e6a7c-7aec-4e58-b22a-82901f820a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Process single file function\n",
    "def process_file(file_type, year, month, day):\n",
    "    schemas = get_schemas()\n",
    "    bronze_path = f'/mnt/project2/bronze/{file_type}/{year}/{month}/{day}'\n",
    "    silver_path = f'/mnt/project2/silver/{file_type}'\n",
    "    \n",
    "    df = spark.read.format('csv').options(header='true').schema(schemas[file_type]).load(bronze_path)\n",
    "    df_no_dup = df.dropDuplicates()\n",
    "    df_clean = clean_data(df_no_dup, file_type)\n",
    "    \n",
    "    df_clean.write.format('parquet').mode('overwrite').save(silver_path)\n",
    "    print(f\"Written to Silver: {file_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caff1a60-844e-431f-80d0-34e7696b62e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4. Read parquet files\n",
    "def read_silver(file_type):\n",
    "    silver_path = f\"/mnt/project2/silver/{file_type}\"\n",
    "    df = spark.read.format('parquet').load(silver_path)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "817288ad-ada4-48b4-848d-53506c0d3003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5. Generate hashkey\n",
    "def generate_hashkey(df):\n",
    "    df_hashkey = df.withColumn(\"hashkey\", crc32(concat(*df.columns)))\n",
    "    return df_hashkey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7d9eb98-3e2b-41df-bc34-cccbefee982d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6. Table Schemas\n",
    "Create table if not exists hive_metastore.scd1.accounts_scd1\n",
    "(\n",
    "  account_id int,\n",
    "  customer_id int,\n",
    "  account_type string, \n",
    "  balance double,\n",
    "  hashkey bigint,\n",
    "  createdby string,\n",
    "  createddate timestamp,\n",
    "  updatedby string,\n",
    "  updateddate timestamp\n",
    ")\n",
    "Location '/mnt/project2/gold/Accounts' ;\n",
    "\n",
    "\n",
    "Create table if not exists hive_metastore.scd1.customers_scd1\n",
    "(\n",
    "  customer_id integer,\n",
    "  first_name string,\n",
    "  last_name string,\n",
    "  address string,\n",
    "  city string,\n",
    "  state string,\n",
    "  zip string,\n",
    "  hashkey long,\n",
    "  createdby string,\n",
    "  createddate timestamp,\n",
    "  updatedby string,\n",
    "  updateddate timestamp\n",
    ")\n",
    "Location '/mnt/project2/gold/Customers' ;\n",
    "\n",
    "Create table if not exists hive_metastore.scd1.loans_payments_scd1\n",
    "(\n",
    "  payment_id int,\n",
    "  loan_id int,\n",
    "  payment_date date,\n",
    "  payment_amount double,\n",
    "  hashkey long,\n",
    "  createdby string,\n",
    "  createddate timestamp,\n",
    "  updatedby string,\n",
    "  updateddate timestamp\n",
    ")\n",
    "Location '/mnt/project2/gold/Loan_Payments';\n",
    "\n",
    "Create table if not exists hive_metastore.scd1.loans_scd1\n",
    "(\n",
    "  loan_id int,\n",
    "  customer_id int,\n",
    "  loan_amount double,\n",
    "  interest_rate double,\n",
    "  loan_term int,\n",
    "  hashkey long,\n",
    "  createdby string,\n",
    "  createddate timestamp,\n",
    "  updatedby string,\n",
    "  updateddate timestamp\n",
    ")\n",
    "Location '/mnt/project2/gold/Loans';\n",
    "\n",
    "Create table if not exists hive_metastore.scd1.transactions_scd1\n",
    "(\n",
    "  transaction_id int,\n",
    "  account_id int,\n",
    "  transaction_date date,\n",
    "  transaction_amount double,\n",
    "  transaction_type string,\n",
    "  hashkey long,\n",
    "  createdby string,\n",
    "  createddate timestamp,\n",
    "  updatedby string,\n",
    "  updateddate timestamp\n",
    ")\n",
    "Location '/mnt/project2/gold/Transactions';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a91d7917-f87a-44be-8831-017b54cffcae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#7. Merge \n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, lit, current_timestamp\n",
    "\n",
    "def merge_new_data(file_type, src_df):\n",
    "    # Gold path\n",
    "    gold_path = f\"/mnt/project2/gold/{file_type}\"\n",
    "    \n",
    "    join_keys = {\n",
    "        'Accounts': 'account_id',\n",
    "        'Customers': 'customer_id',\n",
    "        'Loan_Payments': 'payment_id',\n",
    "        'Loans': 'loan_id',\n",
    "        'Transactions': 'transaction_id'\n",
    "    }\n",
    "    \n",
    "    join_key = join_keys[file_type]\n",
    "\n",
    "    # Gold table to df\n",
    "    delta_tbl = DeltaTable.forPath(spark, gold_path)\n",
    "    tgt_df = delta_tbl.toDF()\n",
    "\n",
    "    # Get only new or changed data\n",
    "    src = src_df.alias('src')\n",
    "    tgt = tgt_df.alias('tgt')\n",
    "\n",
    "    join_condition = col(f'src.{join_key}') == col(f'tgt.{join_key}')\n",
    "    filter_condition = (col(f'tgt.{join_key}').isNull()) | (col('src.hashkey') != col('tgt.hashkey'))\n",
    "\n",
    "    filtered_src = src.join(tgt, join_condition, 'left').filter(filter_condition).select('src.*')\n",
    "\n",
    "    # Add columns in src list\n",
    "    src_cols = []  \n",
    "    for col_name in filtered_src.columns:\n",
    "        src_cols.append(col_name)  # Add each column name\n",
    "\n",
    "    # Update expression \n",
    "    update_expr = {}  # Start with an empty dictionary\n",
    "    for col_name in src_cols:\n",
    "        if col_name not in ['createdby', 'createddate']:     #exclude createdby and createddate\n",
    "            update_expr[col_name] = f\"src.{col_name}\"         #colname:src_colname\n",
    "\n",
    "    # Overwrite values for updatedby and updateddate\n",
    "    update_expr['updatedby'] = lit('Databricks-updated')\n",
    "    update_expr['updateddate'] = current_timestamp()\n",
    "\n",
    "    # Insert expression \n",
    "    insert_expr = {}  # Start with an empty dictionary\n",
    "    for col_name in src_cols:\n",
    "        insert_expr[col_name] = f\"src.{col_name}\"\n",
    "\n",
    "    # Override audit fields for insert\n",
    "    insert_expr['createdby'] = lit('Databricks')\n",
    "    insert_expr['createddate'] = current_timestamp()\n",
    "    insert_expr['updatedby'] = lit('Databricks')\n",
    "    insert_expr['updateddate'] = current_timestamp()\n",
    "\n",
    "    # Merge into Gold Delta table\n",
    "    delta_tbl.alias('tgt').merge(\n",
    "        filtered_src.alias('src'),\n",
    "        join_condition\n",
    "    ).whenMatchedUpdate(set=update_expr\n",
    "    ).whenNotMatchedInsert(values=insert_expr\n",
    "    ).execute()\n",
    "\n",
    "    print(f\"Merged data into Gold table: {file_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57ca9a3c-9951-4da4-90be-904195da3068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#8 Join Function\n",
    "def perform_final_merge(files_found):\n",
    "    # Required files\n",
    "    required_files = ['Accounts', 'Customers', 'Loan_Payments', 'Loans', 'Transactions']\n",
    "\n",
    "    # Check if all required files are available\n",
    "    if all(file in files_found for file in required_files):\n",
    "        print(\"All required files available.\")\n",
    "\n",
    "        accounts_df = spark.read.format('parquet').load(f'/mnt/project2/silver/Accounts')\n",
    "        customers_df = spark.read.format('parquet').load(f'/mnt/project2/silver/Customers')\n",
    "        transactions_df = spark.read.format('parquet').load(f'/mnt/project2/silver/Transactions')\n",
    "        loan_payments_df = spark.read.format('parquet').load(f'/mnt/project2/silver/Loan_Payments')\n",
    "        loans_df = spark.read.format('parquet').load(f'/mnt/project2/silver/Loans')\n",
    "\n",
    "        # Merge logic\n",
    "        merged_df = accounts_df \\\n",
    "            .join(customers_df, 'customer_id', 'inner') \\\n",
    "            .join(transactions_df, 'account_id', 'inner')\n",
    "\n",
    "        merged_loan = loan_payments_df \\\n",
    "            .join(loans_df, 'loan_id', 'inner')\n",
    "\n",
    "        merged_final = merged_df \\\n",
    "            .join(merged_loan, 'customer_id', 'inner') \\\n",
    "            .select(\n",
    "                'account_id', \n",
    "                'customer_id', \n",
    "                'payment_id', \n",
    "                'loan_id', \n",
    "                'transaction_id', \n",
    "                'transaction_date', \n",
    "                'transaction_amount', \n",
    "                'payment_amount', \n",
    "                'payment_date', \n",
    "                'loan_amount', \n",
    "                'balance'\n",
    "            )\n",
    "\n",
    "        # Write merged data to Gold (Delta)\n",
    "        merged_final.write.format('parquet').mode('overwrite').save('/mnt/project2/gold/MergedFinal')\n",
    "        print(\"Merged data written to Gold layer\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Skipping merge logic. Not all required files are available.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
